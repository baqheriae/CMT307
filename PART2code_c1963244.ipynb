{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Surface\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Surface\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Surface\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import sklearn\n",
    "import operator\n",
    "import requests\n",
    "from string import punctuation\n",
    "from os import listdir\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "from collections import Counter\n",
    "from nltk.util import ngrams\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we start by reading each of the files provided, and saving them in the relevant variables\n",
    "#note: I've added a certain text encoding as I experienced errors when useing the default...\n",
    "\n",
    "\n",
    "#first read train posivie and negative files, and add each to a matrix\n",
    "path_train_pos='./IMDb/train/imdb_train_pos.txt'\n",
    "dataset_train_pos=open(path_train_pos,encoding='utf8').readlines()\n",
    "\n",
    "path_train_neg='./IMDb/train/imdb_train_neg.txt'\n",
    "dataset_train_neg=open(path_train_neg,encoding='utf8').readlines()\n",
    "\n",
    "#now read test positive and negative files, and add each to a matrix\n",
    "path_test_pos='./IMDb/test/imdb_test_pos.txt'\n",
    "dataset_test_pos=open(path_test_pos,encoding='utf8').readlines()\n",
    "\n",
    "path_test_neg='./IMDb/test/imdb_test_neg.txt'\n",
    "dataset_test_neg=open(path_test_neg,encoding='utf8').readlines()\n",
    "\n",
    "#now read develop positive and negative files, and add each to a matrix\n",
    "path_dev_pos='./IMDb/dev/imdb_dev_pos.txt'\n",
    "dataset_dev_pos=open(path_dev_pos,encoding='utf8').readlines()\n",
    "\n",
    "path_dev_neg='./IMDb/dev/imdb_dev_neg.txt'\n",
    "dataset_dev_neg=open(path_dev_neg,encoding='utf8').readlines()\n",
    "\n",
    "#dataset_file2=pd.read_csv('C:/Users/Surface/Documents/Applied Machine Learning/datasets_coursework1/IMDb/train/imdb_train_neg.txt',encoding='utf8')\n",
    "#print(dataset_file2.head(5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive reviews from the training dataset:\n",
      "\n",
      "For fans of Chris Farley, this is probably his best film. David Spade plays the perfect cynical, sarcastic yin to Farley's \"Baby Huey\" yang. Farley achieves strokes of comic genius in his monologues, like the \"Let's say you're driving along the road with your family...\" bit, the \"Jo-Jo the Idiot Circus Boy with a pretty new pet, (his possible sale)\" speech, or the \"Glue-sniffing Guarantee fairy\" brake pad sale. The sappy moments in the film contrast sharply with Farley and Spade's shenanigans. Even after many viewings, it's still fun to see Farley pour everything he had into the role. \"Richard, what's HAPPENING to me?!?!\"\n",
      "\n",
      "\n",
      "   ------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#print the first 3 entries of positive train, file name: imdb_train_pos to make sure file is opened successfully...\n",
    "print (\"Positive reviews from the training dataset:\\n\")\n",
    "for i in dataset_train_pos[:1]:\n",
    "  print (i)\n",
    "print (\"\\n   ------------------------\\n\")  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now we join pos and neg of each set into 1 set:\n",
    "\n",
    "new_train_set=[]\n",
    "for pos_review in dataset_train_pos:\n",
    "  new_train_set.append((pos_review,1))\n",
    "for neg_review in dataset_train_neg:\n",
    "  new_train_set.append((neg_review,0))\n",
    "\n",
    "\n",
    "new_test_set=[]\n",
    "for pos_review in dataset_test_pos:\n",
    "  new_test_set.append((pos_review,1))\n",
    "for neg_review in dataset_test_neg:\n",
    "  new_test_set.append((neg_review,0))\n",
    "\n",
    "\n",
    "new_dev_set=[]\n",
    "for pos_review in dataset_dev_pos:\n",
    "  new_dev_set.append((pos_review,1))\n",
    "for neg_review in dataset_dev_neg:\n",
    "  new_dev_set.append((neg_review,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING SET\n",
      "Size training set: 15000\n",
      "('For fans of Chris Farley, this is probably his best film. David Spade plays the perfect cynical, sarcastic yin to Farley\\'s \"Baby Huey\" yang. Farley achieves strokes of comic genius in his monologues, like the \"Let\\'s say you\\'re driving along the road with your family...\" bit, the \"Jo-Jo the Idiot Circus Boy with a pretty new pet, (his possible sale)\" speech, or the \"Glue-sniffing Guarantee fairy\" brake pad sale. The sappy moments in the film contrast sharply with Farley and Spade\\'s shenanigans. Even after many viewings, it\\'s still fun to see Farley pour everything he had into the role. \"Richard, what\\'s HAPPENING to me?!?!\"\\n', 1)\n",
      "    \n",
      "-------\n",
      "\n",
      "TEST SET\n",
      "Size development set: 5000\n",
      "('After 10 viewings in 20 years I too think this was the Crazy Gang\\'s best effort on film, with more cohesion in the plot than their next best, \"Alf\\'s Button Afloat\". They were indeed a crazy trio of double acts thrown together mainly on stage, sometimes in front of royalty, until Chesney Allen retired in the \\'40\\'s through \"ill-health\". He outlived them all by years. Apparently they were just as mad outside \"work\", regularly playing practical jokes on one another.<br /><br />The Six Wonder Boys troupe head for I\\'ll-Get-Her-To-Tell-Me (Alaska) to dig for the gold that was being found there. It seemed a better idea than going to Mansfield ... because they\\'d been there. When they get to Red Gulch they find their information was a mere 40 years out of date - they thought that the chips that were in the guilty newspaper they\\'d read tasted funny. But by then it doesn\\'t matter as they\\'ve all fallen in love with Snow White and want to help her grandad find his long lost stash of gold. Baddie Bill \"M\" McGrew wants it himself however.<br /><br />The number of verbal and visual puns is astonishing, but most of them will probably only make sense(?) to Brits and ex-pats interested in seeing \\'30\\'s British b&w comedies. Imho nearly all of the gags and routines work, including the Gold If patter between Bud & Chesney and the \"Whistle While You Work\" pastiche - even the \"Always Getting Our Man\" Mountie inserts. A marvellous little film, in a rather tired looking condition but utterly recommended.\\n', 1)\n",
      "    \n",
      "-------\n",
      "\n",
      "DEVELOPMENT SET\n",
      "Size test set: 5000\n",
      "('This is the greatest movie if you want inspiration on following your heart and never giving up on your dream. Elizabeth Taylor is Velvet and in her prime (of her childhood, at least), Mickey Rooney is a cynical friend who eventually becomes her trainer and they go off to the Grand National steeplechase with her beloved horse \"the Pi\"--short for \"Pirate\"--only to have Velvet become the jockey and have a chance at victory. To those of you who have not seen it yet, I won\\'t give away the ending but you should see it and once you do you\\'ll love it. Notice a very young Angela Lansbury as Velvet\\'s eldest sister.\\n', 1)\n"
     ]
    }
   ],
   "source": [
    "#here we just print the first entry for each set to make sure the mering was successful\n",
    "print (\"TRAINING SET\")\n",
    "print (\"Size training set: \"+str(len(new_train_set)))\n",
    "for example in new_train_set[:1]:\n",
    "    print (example)\n",
    "print (\"    \\n-------\\n\")\n",
    "print (\"TEST SET\")\n",
    "print (\"Size development set: \"+str(len(new_dev_set)))\n",
    "for example in new_dev_set[:1]:\n",
    "  print (example)\n",
    "print (\"    \\n-------\\n\")\n",
    "print (\"DEVELOPMENT SET\")\n",
    "print (\"Size test set: \"+str(len(new_test_set)))\n",
    "for example in new_test_set[:1]:\n",
    "  print (example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29803\n",
      "[('good', 8100), ('great', 5386), ('bad', 5340), ('many', 4127), ('much', 4088), ('little', 3368), ('first', 2927), ('real', 2718), ('old', 2334), ('new', 2268), ('funny', 2089), ('big', 2028), ('young', 1951), ('whole', 1748), ('br', 1745), ('original', 1706), ('ive', 1683), ('last', 1680), ('u', 1543), ('main', 1414), ('different', 1349), ('sure', 1310), ('american', 1289), ('special', 1286), ('dont', 1275), ('true', 1239), ('black', 1232), ('hard', 1208), ('second', 1169), ('high', 1159), ('short', 1151), ('poor', 1146), ('cant', 1104), ('classic', 1076), ('give', 1066), ('excellent', 1039), ('beautiful', 1029), ('right', 1028), ('full', 1018), ('human', 978), ('nice', 972), ('stupid', 965), ('interesting', 960), ('dead', 941), ('terrible', 925), ('wrong', 920), ('im', 918), ('enough', 900), ('long', 890), ('small', 888)]\n"
     ]
    }
   ],
   "source": [
    "from string import punctuation\n",
    "from os import listdir\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "\n",
    "# turn a doc into clean tokens\n",
    "def clean_doc(doc):\n",
    "    # split into tokens by white space\n",
    "    tokens = doc.split()\n",
    "    # remove punctuation from each token\n",
    "    table = str.maketrans('', '', punctuation)\n",
    "    tokens = [w.translate(table) for w in tokens]\n",
    "    # remove remaining tokens that are not alphabetic\n",
    "    tokens = [word for word in tokens if word.isalpha()]\n",
    "    # filter out stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [w for w in tokens if not w in stop_words]\n",
    "    # filter out short tokens\n",
    "    tokens = [word for word in tokens if len(word) > 1]\n",
    "    list_tokens=[]\n",
    "    for token in tokens:\n",
    "        list_tokens.append(lemmatizer.lemmatize(token).lower())\n",
    "    # the following part shall the tokens in and return POS tuple with word type tag, \n",
    "    # we shall then be taking adjectives and verbs only\n",
    "    test = nltk.pos_tag(list_tokens)\n",
    "    final_tokens = [a[0] for a in test if (a[1] == 'JJ' or a[1] == 'VERB')]\n",
    "    return final_tokens\n",
    " \n",
    "# load doc and add to vocab\n",
    "def add_doc_to_vocab(filename, vocab):\n",
    "    # clean doc\n",
    "    tokens = clean_doc(filename)\n",
    "    # update counts\n",
    "    vocab.update(tokens)\n",
    " \n",
    " \n",
    "# define vocab\n",
    "vocab = Counter()\n",
    "# add all docs to vocab\n",
    "for instance in ([i[0] for i in new_train_set]):\n",
    "    add_doc_to_vocab(instance, vocab)\n",
    "\n",
    "# print the size of the vocab\n",
    "print(len(vocab))\n",
    "# print the top 50 words in the vocab\n",
    "print(vocab.most_common(50))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this function shall take care tokenizing the text, normalizing and cleaning as per clean_doc function, and vectorinzation as well.\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(tokenizer=clean_doc,ngram_range=(1,2),max_features=500)\n",
    "matrix = vectorizer.fit_transform([i[0] for i in new_train_set])\n",
    "a = matrix.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to train the model is: 83.504465 seconds\n"
     ]
    }
   ],
   "source": [
    "#now we train our model, this shall take some time.\n",
    "import time\n",
    "start = time.time()\n",
    "X_train=a\n",
    "Y_train=[i[1] for i in new_train_set]\n",
    "svm_clf=sklearn.svm.SVC(kernel=\"linear\",gamma='auto')\n",
    "svm_clf.fit(np.asarray(X_train),np.asarray(Y_train))\n",
    "end = time.time()\n",
    "\n",
    "print(\"Time to train the model is: %f seconds\"%(float(end)- float(start)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.79      0.77      2394\n",
      "           1       0.80      0.77      0.78      2606\n",
      "\n",
      "    accuracy                           0.78      5000\n",
      "   macro avg       0.78      0.78      0.78      5000\n",
      "weighted avg       0.78      0.78      0.78      5000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#here we shall start checking the performance or our model using the development set\n",
    "#we start with the trained model vectorizer that uses most common 500 words and ngrams = 2\n",
    "\n",
    "X_dev = vectorizer.transform([i[0] for i in new_dev_set]).toarray()\n",
    "predictions = svm_clf.predict(X_dev)\n",
    "print(sklearn.metrics.classification_report(predictions,[i[1] for i in new_dev_set]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.81      0.79      2370\n",
      "           1       0.82      0.79      0.80      2630\n",
      "\n",
      "    accuracy                           0.80      5000\n",
      "   macro avg       0.80      0.80      0.80      5000\n",
      "weighted avg       0.80      0.80      0.80      5000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#we notice that the accuracy is not bad, but it could be improved. \n",
    "#we will use the development set to try some fine tuning, here we will consider 2000 words, and ngrams = 3\n",
    "\n",
    "\n",
    "vectorizer2 = TfidfVectorizer(tokenizer=clean_doc,ngram_range=(1,3),max_features=2000)\n",
    "matrix = vectorizer2.fit_transform(np.asarray([i[0] for i in new_train_set]))\n",
    "X_train = matrix.toarray()\n",
    "Y_train=[i[1] for i in new_train_set]\n",
    "\n",
    "svm_clf2=sklearn.svm.SVC(kernel=\"linear\",gamma='auto')\n",
    "svm_clf2.fit(np.asarray(X_train),np.asarray(Y_train))\n",
    "\n",
    "X_dev = vectorizer2.transform([i[0] for i in new_dev_set]).toarray()\n",
    "predictions = svm_clf2.predict(X_dev)\n",
    "print(sklearn.metrics.classification_report(predictions,[i[1] for i in new_dev_set]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#here we shall try to reduce the dimentionality of our model, by selecting the features with highest score\n",
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "\n",
    "vectorizer_f = TfidfVectorizer(tokenizer=clean_doc,ngram_range=(1,2),max_features=5000)\n",
    "X_train = vectorizer_f.fit_transform([i[0] for i in new_train_set]).toarray()\n",
    "Y_train = np.asarray([i[1] for i in new_train_set])\n",
    "\n",
    "X_feature_best = SelectKBest(chi2, k=500).fit(X_train, Y_train)\n",
    "X_train_feature_best = X_feature_best.transform(X_train)\n",
    "\n",
    "X_test = vectorizer_f.transform([i[0] for i in new_test_set]).toarray()\n",
    "X_test_feature_best =  X_feature_best.transform(X_test)\n",
    "\n",
    "Y_test = np.asarray([i[1] for i in new_test_set])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.83      0.79      2284\n",
      "           1       0.84      0.77      0.81      2716\n",
      "\n",
      "    accuracy                           0.80      5000\n",
      "   macro avg       0.80      0.80      0.80      5000\n",
      "weighted avg       0.80      0.80      0.80      5000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#finally we train our model and predict the results using the test dataset\n",
    "svm_clf_3=sklearn.svm.SVC(kernel=\"linear\",gamma='auto')\n",
    "svm_clf_3.fit(np.asarray(X_train_feature_best),Y_train)\n",
    "predictions_feature_best = svm_clf_3.predict(X_test_feature_best)\n",
    "print(sklearn.metrics.classification_report(predictions_feature_best,Y_test))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.799\n",
      "Recall: 0.797\n",
      "F1-Score: 0.797\n",
      "Accuracy: 0.797\n"
     ]
    }
   ],
   "source": [
    "#we are going to measure precision, recall, F1 score and Accuracy here\n",
    "from sklearn.metrics import precision_score,recall_score,f1_score,accuracy_score\n",
    "Y_test_gold = Y_test\n",
    "Y_text_predictions = predictions_feature_best\n",
    "precision=precision_score(Y_test_gold, Y_text_predictions, average='macro')\n",
    "recall=recall_score(Y_test_gold, Y_text_predictions, average='macro')\n",
    "f1=f1_score(Y_test_gold, Y_text_predictions, average='macro')\n",
    "accuracy=accuracy_score(Y_test_gold, Y_text_predictions)\n",
    "\n",
    "print (\"Precision: \"+str(round(precision,3)))\n",
    "print (\"Recall: \"+str(round(recall,3)))\n",
    "print (\"F1-Score: \"+str(round(f1,3)))\n",
    "print (\"Accuracy: \"+str(round(accuracy,3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1885  616]\n",
      " [ 399 2100]]\n"
     ]
    }
   ],
   "source": [
    "#we could check the confusion matrix to verify tn, fp, fn, tp\n",
    "from sklearn.metrics import confusion_matrix\n",
    "print (confusion_matrix(Y_test_gold, Y_text_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
